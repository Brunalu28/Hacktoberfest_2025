{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaLX_FB22cJo"
      },
      "source": [
        "### CÉLULA 0: Importação de Bibliotecas Essenciais 🛠️"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4lST9KP0F4g",
        "outputId": "54e0b8e9-d337-43f8-db59-04728d229cf6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/323.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.9/323.9 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hpypdf instalado com sucesso!\n"
          ]
        }
      ],
      "source": [
        "!pip install -qU pypdf\n",
        "print(\"pypdf instalado com sucesso!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wPK9ZTmQ0KAv"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dp0fNolJfO0e",
        "outputId": "7871395d-73f9-46e3-a4c2-ffa11834d12c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/107.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.8/107.8 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/467.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m467.1/467.1 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.4/155.4 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.1/46.1 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.6/207.6 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qU langchain # LangChain é útil para abstrair a interação com o LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M850t4mYgG-z",
        "outputId": "fc9ebb06-8cff-4024-9ee8-d42360fda886"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "llama-cpp-python instalado com sucesso!\n",
            "Dependências instaladas.\n"
          ]
        }
      ],
      "source": [
        "!pip install -qU llama-cpp-python\n",
        "# Verifica se a instalação do llama-cpp-python foi bem-sucedida\n",
        "try:\n",
        "    from llama_cpp import Llama\n",
        "    print(\"llama-cpp-python instalado com sucesso!\")\n",
        "except ImportError:\n",
        "    print(\"Erro na instalação do llama-cpp-python. Verifique as saídas acima.\")\n",
        "    print(\"Se a instalação com CUDA falhou, tente comentar a primeira linha de instalação e descomentar a segunda.\")\n",
        "\n",
        "print(\"Dependências instaladas.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_YP5GRRRhMHi",
        "outputId": "2b006262-e9e7-45db-d7b4-de6979395162"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/566.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m563.2/566.1 kB\u001b[0m \u001b[31m68.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Instala a biblioteca huggingface_hub para baixar modelos\n",
        "!pip install -qU huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xI4wjLbGnyp7",
        "outputId": "e2c74c3f-538d-4110-869d-0ae273e6595a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m147.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m64.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -qU langchain-community # Necessário para LlamaCpp e outras integrações da LangChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305,
          "referenced_widgets": [
            "49f1e8adc9e143299dd0cd55c4ce0092",
            "46c9c266c45f44c8bce85308c3bffde6",
            "d38f81a7fb79460f977e70fd6baefe05",
            "024513b70b5a4d4d9ededc3a11af28fd",
            "20f6eb04dccb4b478abaf581829a83e8",
            "0549998d037741a0a654223f7daf46b5",
            "62a7b8267f9c478284ca315350c72692",
            "b1811f8d11c943819b3c949a643d6183",
            "d9066538d1cd4150be9b008734320ad4",
            "2b43164e391b4b2d99b351354f99b785",
            "ed80012eb9994d8591a591133ce617d7"
          ]
        },
        "id": "c0meTqoghUAp",
        "outputId": "9d366e88-58fd-4205-cde9-45477607d59d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Baixando o modelo mistral-7b-instruct-v0.2.Q4_K_M.gguf de TheBloke/Mistral-7B-Instruct-v0.2-GGUF...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:979: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\n",
            "For more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "49f1e8adc9e143299dd0cd55c4ce0092",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "mistral-7b-instruct-v0.2.Q4_K_M.gguf:   0%|          | 0.00/4.37G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Download do modelo concluído!\n",
            "Caminho do modelo: ./models/mistral-7b-instruct-v0.2.Q4_K_M.gguf\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "import os\n",
        "\n",
        "model_name = \"mistral-7b-instruct-v0.2.Q4_K_M.gguf\" # Altere para o nome do novo arquivo GGUF\n",
        "model_repo_id = \"TheBloke/Mistral-7B-Instruct-v0.2-GGUF\" # Altere para o novo repositório\n",
        "# Define o diretório onde o modelo será salvo\n",
        "model_dir = \"./models\"\n",
        "os.makedirs(model_dir, exist_ok=True) # Cria o diretório se não existir\n",
        "\n",
        "# Caminho completo para o modelo\n",
        "model_path = os.path.join(model_dir, model_name)\n",
        "\n",
        "# Verifica se o modelo já existe para evitar download repetido\n",
        "if not os.path.exists(model_path):\n",
        "    print(f\"Baixando o modelo {model_name} de {model_repo_id}...\")\n",
        "    hf_hub_download(\n",
        "        repo_id=model_repo_id,\n",
        "        filename=model_name,\n",
        "        local_dir=model_dir,\n",
        "        local_dir_use_symlinks=False # Importante para Colab\n",
        "    )\n",
        "    print(\"Download do modelo concluído!\")\n",
        "else:\n",
        "    print(f\"O modelo {model_name} já existe em {model_dir}. Pulando o download.\")\n",
        "\n",
        "print(f\"Caminho do modelo: {model_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tuALBJqwhiLu",
        "outputId": "091ab842-6e6a-4e12-d93b-2e923a42baa0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "llama_context: n_batch is less than GGML_KQ_MASK_PAD - increasing to 64\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Modelo LLM inicializado com sucesso!\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.llms import LlamaCpp\n",
        "from langchain_core.callbacks.manager import CallbackManager\n",
        "from langchain_core.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "\n",
        "# Configura o callback para streaming da saída\n",
        "#callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
        "\n",
        "\n",
        "llm = LlamaCpp(\n",
        "    model_path=model_path, # Caminho para o modelo GGUF\n",
        "    n_ctx=2048,           # Contexto máximo de tokens (ajuste conforme o modelo)\n",
        "    n_gpu_layers=-1,      # Descarrega todas as camadas para a GPU (se disponível)\n",
        "    n_threads=os.cpu_count(), # Usa o número de threads da CPU disponível\n",
        "    temperature=0.3,      # Reduzido para tornar as respostas mais determinísticas\n",
        "    max_tokens=100,       # Ajustado para garantir que a resposta completa seja gerada\n",
        "    verbose=False          # Saída detalhada\n",
        ")\n",
        "#callback_manager=callback_manager, # Para ver a saída em tempo real\n",
        "print(\"\\nModelo LLM inicializado com sucesso!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fIpWilgovdu"
      },
      "source": [
        "### CÉLULA 1: Importação de Bibliotecas Essenciais 📚\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cGSeqrwiihbZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import PromptTemplate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2r41ydKo-Qu"
      },
      "source": [
        "### CÉLULA 2: Configuração do Webhook do Discord ⚙️\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pjvUeyEZiIyk"
      },
      "outputs": [],
      "source": [
        "# Substitua este URL pelo Webhook URL que você copiou do Discord\n",
        "DISCORD_WEBHOOK_URL_PLACEHOLDER = \"COLE SEU WEBHOOK AQUI\"\n",
        "\n",
        "DISCORD_WEBHOOK_URL = os.getenv(\"DISCORD_WEBHOOK_URL\", DISCORD_WEBHOOK_URL_PLACEHOLDER)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Mu1xlh3pKCp"
      },
      "source": [
        "### CÉLULA 3: Cadeia de Análise PDFs (LLMChain) 🧠\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "myYWFXjtiMmi"
      },
      "outputs": [],
      "source": [
        "slide_summary_prompt = PromptTemplate.from_template(\n",
        "    \"\"\"Você é um assistente de estudo prestativo.\n",
        "Crie um resumo conciso, com no máximo 20 linhas, do seguinte texto de slide.\n",
        "O resumo deve focar nas ideias centrais, usar linguagem simples e ter um tom informativo e amigável para auxiliar estudantes.\n",
        "\n",
        "Texto do Slide: \"{texto_do_slide}\"\n",
        "\n",
        "Resumo:\"\"\"\n",
        ")\n",
        "# Remove o 'stop' para permitir um resumo mais longo\n",
        "slide_summary_chain = slide_summary_prompt | llm | StrOutputParser()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "peAo2IyYpU8q"
      },
      "source": [
        "### CÉLULA 4: Cadeia de Sugestão de Resposta ao Cliente (LLMChain) 💬\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FFIIK8isiPQQ"
      },
      "outputs": [],
      "source": [
        "key_concepts_prompt = PromptTemplate.from_template(\n",
        "    \"\"\"Analise o seguinte texto de slide e identifique os 3-5 conceitos mais importantes.\n",
        "Liste cada conceito em uma linha separada, começando com um hífen.\n",
        "\n",
        "Texto do Slide: \"{texto_do_slide}\"\n",
        "\n",
        "Conceitos-Chave:\"\"\"\n",
        ")\n",
        "key_concepts_chain = key_concepts_prompt | llm | StrOutputParser()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xzu9txZ9pcA4"
      },
      "source": [
        "### CÉLULA 5: Cadeia de Avaliação de Prioridade (LLMChain) 🚀\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4aJMKwAtiR6s"
      },
      "outputs": [],
      "source": [
        "study_questions_prompt = PromptTemplate.from_template(\n",
        "    \"\"\"Com base no seguinte texto de slide, crie 2-3 perguntas de estudo que ajudem os estudantes a testar seu entendimento do conteúdo.\n",
        "As perguntas devem ser claras e focadas nos pontos principais.\n",
        "\n",
        "Texto do Slide: \"{texto_do_slide}\"\n",
        "\n",
        "Perguntas de Estudo:\"\"\"\n",
        ")\n",
        "study_questions_chain = study_questions_prompt | llm | StrOutputParser()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bS6j1YWOpeoG"
      },
      "source": [
        "### CÉLULA 6: Função para enviar mensagens ao Discord 🚀\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Adng5CbviUMO"
      },
      "outputs": [],
      "source": [
        "def send_discord_message(text: str, webhook_url: str, username: str = \"Assistente de Estudos\", avatar_url: str = None):\n",
        "    \"\"\"\n",
        "    Envia uma mensagem de texto para o Discord usando um webhook.\n",
        "    \"\"\"\n",
        "    if not webhook_url or \"discord.com/api/webhooks\" not in webhook_url:\n",
        "        print(f\"ERRO: Discord Webhook URL inválido ou não configurado. Mensagem não enviada: '{text}'\")\n",
        "        return\n",
        "\n",
        "    payload = {\n",
        "        \"content\": text,\n",
        "        \"username\": username,\n",
        "        \"avatar_url\": avatar_url\n",
        "    }\n",
        "    try:\n",
        "        response = requests.post(webhook_url, json=payload)\n",
        "        response.raise_for_status() # Lança exceção para erros HTTP (4xx ou 5xx)\n",
        "        print(f\"\\n[INTEGRAÇÃO DISCORD] Mensagem enviada com sucesso para o Discord!\")\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"\\n[ERRO DISCORD] Falha ao enviar mensagem para o Discord: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfmZHH958nI4"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def preprocess_glued_text(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Tenta corrigir palavras grudadas em um texto, inserindo espaços.\n",
        "\n",
        "    Argumentos:\n",
        "        text (str): O texto extraído do PDF.\n",
        "\n",
        "    Retorna:\n",
        "        str: O texto pré-processado.\n",
        "    \"\"\"\n",
        "    # 1. Trata quebras de linha e retornos de carro que podem ter vindo da extração\n",
        "    processed_text = text.replace('\\n', ' ').replace('\\r', ' ')\n",
        "\n",
        "    # 2. Insere espaço entre letra minúscula e letra MAIÚSCULA\n",
        "    # Ex: 'umEndereço' -> 'um Endereço'\n",
        "    processed_text = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', processed_text)\n",
        "\n",
        "    # 3. Insere espaço entre letra e número\n",
        "    # Ex: 'pergunta1' -> 'pergunta 1'\n",
        "    processed_text = re.sub(r'([a-zA-Z])([0-9])', r'\\1 \\2', processed_text)\n",
        "\n",
        "    # 4. Insere espaço entre número e letra\n",
        "    # Ex: '1pergunta' -> '1 pergunta'\n",
        "    processed_text = re.sub(r'([0-9])([a-zA-Z])', r'\\1 \\2', processed_text)\n",
        "\n",
        "    # 5. Remove espaços duplos ou múltiplos criados pelas substituições\n",
        "    processed_text = re.sub(r'\\s+', ' ', processed_text).strip()\n",
        "\n",
        "    return processed_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GqyQPAXT0U-h"
      },
      "outputs": [],
      "source": [
        "def extract_text_from_pdf(pdf_path: str) -> str:\n",
        "    \"\"\"\n",
        "    Carrega um PDF, extrai, concatena e pré-processa o texto de todas as páginas.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # 1. Carrega o documento usando PyPDFLoader\n",
        "        loader = PyPDFLoader(pdf_path)\n",
        "        pages = loader.load()\n",
        "\n",
        "        # 2. Concatena o conteúdo das páginas (texto bruto)\n",
        "        # Note: A quebra de linha '\\n' aqui é do próprio texto do PDF.\n",
        "        # O preprocessamento vai tratar isso.\n",
        "        full_text_bruto = \"\\n\".join(page.page_content for page in pages)\n",
        "\n",
        "        # 3. *** PASSO ADICIONAL: PRÉ-PROCESSAR O TEXTO ***\n",
        "        full_text_processado = preprocess_glued_text(full_text_bruto)\n",
        "        # *************************************************\n",
        "\n",
        "        print(f\"Sucesso! Texto extraído e pré-processado de {len(pages)} páginas do PDF.\")\n",
        "\n",
        "        # Retorna o texto LIMPO\n",
        "        return full_text_processado\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"ERRO: Arquivo PDF não encontrado no caminho: {pdf_path}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"ERRO ao processar o PDF: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCfHOwlPpzS7"
      },
      "source": [
        "### CÉLULA 7: Célula de Testes com Feedbacks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXJOElN7iW27",
        "outputId": "bcce4db8-c309-415f-aa0d-860f1664cab6"
      },
      "outputs": [],
      "source": [
        "# Célula de Testes com Materiais de Estudo (AGORA COM PDF)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # 1. Extrair e Processar o texto do PDF\n",
        "    pdf_file_name = \"ADICIONE O SEU PDF AQUI\"\n",
        "    # O texto retornado já está PRÉ-PROCESSADO!\n",
        "    texto_do_pdf_limpo = extract_text_from_pdf(pdf_file_name)\n",
        "\n",
        "    if texto_do_pdf_limpo:\n",
        "\n",
        "        # O texto de entrada do LLM é o texto limpo\n",
        "        texto_para_llm = texto_do_pdf_limpo[:1500]\n",
        "\n",
        "        print(f\"\\n--- Processando Texto Extraído do PDF ({pdf_file_name}) ---\\n\")\n",
        "        print(f\"Texto Original (Início):\\n{texto_para_llm[:500]}...\\n\") # Mostra o começo\n",
        "\n",
        "        # Passo 1: Resumo do Slide/Texto\n",
        "        resumo_pdf = slide_summary_chain.invoke({\"texto_do_slide\": texto_para_llm}).strip()\n",
        "        print(f\"\\nResumo: {resumo_pdf}\")\n",
        "\n",
        "        # Passo 2: Conceitos-Chave\n",
        "        conceitos_pdf = key_concepts_chain.invoke({\"texto_do_slide\": texto_para_llm}).strip()\n",
        "        print(f\"\\nConceitos-Chave: {conceitos_pdf}\")\n",
        "\n",
        "        # Passo 3: Perguntas de Estudo\n",
        "        perguntas_pdf = study_questions_chain.invoke({\"texto_do_slide\": texto_para_llm}).strip()\n",
        "        print(f\"\\nPerguntas de Estudo: {perguntas_pdf}\")\n",
        "\n",
        "        # Enviar para o Discord\n",
        "        discord_message_pdf = (\n",
        "            f\"**NOVO RESUMO DE ESTUDO (PDF: {pdf_file_name})**\\\\n\"\\\n",
        "            f\"> **Slide Original (Trecho):**\\\\n```\\\\n{texto_para_llm[:300]}...\\\\n```\\\\n\"\\\n",
        "            f\"> **Resumo:**\\\\n```\\\\n{resumo_pdf}\\\\n```\\\\n\"\\\n",
        "            f\"> **Conceitos-Chave:**\\\\n```\\\\n{conceitos_pdf}\\\\n```\\\\n\"\\\n",
        "            f\"> **Perguntas de Estudo:**\\\\n```\\\\n{perguntas_pdf}\\\\n```\"\n",
        "        )\n",
        "        send_discord_message(discord_message_pdf, DISCORD_WEBHOOK_URL, username=f\"Assistente de Estudos - {pdf_file_name}\")\n",
        "\n",
        "    else:\n",
        "        print(\"Não foi possível processar o PDF. Verifique o caminho do arquivo.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "024513b70b5a4d4d9ededc3a11af28fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2b43164e391b4b2d99b351354f99b785",
            "placeholder": "​",
            "style": "IPY_MODEL_ed80012eb9994d8591a591133ce617d7",
            "value": " 4.37G/4.37G [01:08&lt;00:00, 110MB/s]"
          }
        },
        "0549998d037741a0a654223f7daf46b5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20f6eb04dccb4b478abaf581829a83e8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2b43164e391b4b2d99b351354f99b785": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46c9c266c45f44c8bce85308c3bffde6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0549998d037741a0a654223f7daf46b5",
            "placeholder": "​",
            "style": "IPY_MODEL_62a7b8267f9c478284ca315350c72692",
            "value": "mistral-7b-instruct-v0.2.Q4_K_M.gguf: 100%"
          }
        },
        "49f1e8adc9e143299dd0cd55c4ce0092": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_46c9c266c45f44c8bce85308c3bffde6",
              "IPY_MODEL_d38f81a7fb79460f977e70fd6baefe05",
              "IPY_MODEL_024513b70b5a4d4d9ededc3a11af28fd"
            ],
            "layout": "IPY_MODEL_20f6eb04dccb4b478abaf581829a83e8"
          }
        },
        "62a7b8267f9c478284ca315350c72692": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b1811f8d11c943819b3c949a643d6183": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d38f81a7fb79460f977e70fd6baefe05": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b1811f8d11c943819b3c949a643d6183",
            "max": 4368439584,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d9066538d1cd4150be9b008734320ad4",
            "value": 4368439584
          }
        },
        "d9066538d1cd4150be9b008734320ad4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ed80012eb9994d8591a591133ce617d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
